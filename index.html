<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="for the dream">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="for the dream">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="for the dream">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>for the dream</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">for the dream</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/19/mysql/innodb机制详解一：概述及内存模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/19/mysql/innodb机制详解一：概述及内存模型/" itemprop="url">innodb机制详解一：概述、体系架构和内存模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-19T00:00:00+08:00">
                2017-10-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/mysql/" itemprop="url" rel="index">
                    <span itemprop="name">mysql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>关系型数据库在各种企业应用中的重要性和地位都不言而喻，Mysql作为开源关系型数据库中最受欢迎的数据库，不管从任何角度来说，掌握好他都是必须的。而innodb存储引擎，作为mysql5.5版本之后的默认存储引擎，可以说是mysql中最优秀的存储引擎。</p>
<p>关于innodb的一些特性和优点，比如行级锁、事务多段回滚、插入缓冲、double write等等的网上随处可见，作为一名开发人员，即使不是专门的DBA，我认为也必须了解innodb内部运作机制。因为我们常说的一些sql优化等各种优化策略，如果没有一定的理论依据，那只能是听到别人说什么就是什么，没有自己的判断依据，当数据库运行出了问题也无从分析。</p>
<p>为此，也为了巩固自己所学的知识，所以开了Innodb这个博客分类，将自己对Innodb的认知写出来，该系列博客内容主要参考了<code>Mysql技术内幕：innodb存储引擎(第2版)</code>以及<code>Mysql内核:innodb存储引擎</code>两本书。这两本书每本都看了2遍左右，相互应征着学习。在第一次看这2本书的时候，有特别多的困惑和不解的地方，前者主要是从应用角度介绍innodb，后者主要是从源码角度讲解innodb。初次读这2本书的时候，遇到的困惑很多，我也大量查阅了网上的很多资料，也没找到能把这些困惑说清楚的(其实是压根没找到类似的内容和文章，内容都是千篇一律，不知道这些博客作者是没考虑到这些问题还是说这些问题只有小白菜鸟才不懂)。我个人非常尊重这2本书的作者，这两本书是我看过讲Innodb最好最透彻的书，但可能是因为我自己理解能力不够，觉得书中某些地方并没有完全讲清楚，对于一些初步了解innodb的读者来说，肯定会产生不少困惑。因为写书的人水平极高，所以一些站在写书人角度上来说完全不是问题的问题，可能反而成了困扰初学者的地方。</p>
<p>基于这些原因，我会把学习innodb过程中自己的认知以及困惑和解决这些困惑的心得全部用自己最白话的语言写出来，希望能帮助到自己以及其他初学者。</p>
<p>关于Innodb的工作原理，我个人认为作为一名程序员，主要需要理解以下几个模块：innodb体现架构和内存模型、日志文件及表空间、B+树索引、innodb的锁、事务。如果能深刻理解这些模块的原理，我觉得在日常开发中编写和mysql相关的程序时，心里会更加自信和有底气。</p>
<h1 id="innodb体系架构"><a href="#innodb体系架构" class="headerlink" title="innodb体系架构"></a>innodb体系架构</h1><p>关于innodb的体系架构和内存模型，下面这张图展示的非常清晰完整。</p>
<p><img src="/images/mysql/innodb体系架构和内存模型.png" alt=""> </p>
<p>我个人认为，想要理解好innodb的工作原理，必须非常清晰的认识理解innodb的内存模型！在学习innodb的过程中，对我产生的一个非常大的困惑之一就是关于磁盘和内存。在innodb中，有很多概念(比如page、insert buffer、索引等)，都是即存在磁盘中，也可能存在内存中(一般来说是缓冲池中)，而在书中的讲述中，并没有非常细致的讲清楚这些基本问题(可能是因为这些概念太基础了，作者默认读者都懂)，但我觉得应该很多初学者都会困惑。</p>
<p>比如说到insert buffer，到底是指缓冲池中的insert buffer还是磁盘中的insert buffer?我们知道，insert buffer内部也是采用的B+树数据结构，那么在内存中是B+树的数据结构很容易理解，在磁盘中也是存储成B+树结构吗？是怎么存的呢？直接内存镜像dump还是二进制序列化？说到B+树二级索引的合并，到底是怎么合并？依据什么合并？是合并到缓冲池还是直接写入磁盘？</p>
<p>上述这些问题，只是我在学习Innodb过程中的困惑之一，这些内容书中确实没有讲的非常细，只能个人自己摸索，我相信应该会有不少初学者也有类似的困惑。</p>
<p>其实产生这些困惑的原因就在于没有理解好innodb的体系结构，没有理解好innodb的内存模型。想要讲清楚这些概念，不是几句话的事，而且牵扯到的概念也比较多，我的目的就是通过这一系列的文章，将自己对innodb的认知写出来，由于本人水平有限，所以难免有错误的地方，希望有不同意见的朋友能一起讨论，共同进步。</p>
<p>从上图中我们可以看到，Innodb的体系结构可以认为分层3个部分。</p>
<ol>
<li><p>内存模型。内存模型中包括了缓冲池(innodb buffer pool)、其他内存(Additional buffer pool)和日志缓存(log buffer，其实主要是redo log)。</p>
</li>
<li><p>各种线程。innodb的大部分工作都是基于多线程模型实现的，内部有很多种线程，比如Main Thread、IO Thread、Purge Thread、Clean Thread等等。对于不同版本的innodb，具体的线程及线程的负责工作的内容都可能不太一样，但整体差别不是很大，并不影响我们理解innodb的工作模式。</p>
</li>
<li><p>innodb的磁盘存储系统。innodb毕竟是一个mysql存储引擎，作为关系型数据库，数据的完整性是第一位的，所以必然少不了数据的持久化。这部分就决定了innodb的数据以及索引是在磁盘中怎样存储的，同时为了数据容灾等考量，各种日志文件也需要持久化到磁盘，所以innodb的磁盘存储我们主要需要理解：系统表空间存储、用户表空间存储、各种日志文件的存储。</p>
</li>
</ol>
<h1 id="innodb的缓冲池"><a href="#innodb的缓冲池" class="headerlink" title="innodb的缓冲池"></a>innodb的缓冲池</h1><p>我个人认为Innodb Buffer Pool(缓冲池)是innodb内存模型中最重要的概念。简单来说，我们可以理解为缓冲池就是一块普通的内存区域，这块内存区域是归属于innodb来管理的。</p>
<p>由于CPU处理数据的速度和从磁盘读取速度的天差地别，所以为了高速读写数据，innodb必须将磁盘中存储的数据(包括索引和数据以及一些日志文件)读取到内存中来，这块内存就叫做innodb的缓冲池。innodb每次从磁盘中读写数据的最小单位可不是byte哦，也不是扇区这种，而是称为page。一个page默认大小是16KB，一般来说，每次从磁盘中读取数据都是一次性读取或写入多个page。page的具体细节后面的文章在详细讲述，这里我们只需要有一个感性认识即可。</p>
<p>因此，我们可以感性的将缓冲池理解为很多个page的组合。当然，同样是page，也有多种类型的page。比如有的page存的是数据，有的page存的是二级索引，有的page存的是Undo信息(具体什么是undo page后面文章详解)。</p>
<p>既然知道了缓冲池的大概组成，我们来想一个问题，当用户向innodb引擎的表中插入一条数据时，由于最终数据会落到磁盘文件中，那么是直接调用类似与writeFile等等的函数向某个磁盘文件中在某个合适的位置(或者是末尾追加)写入一条数据吗？答案肯定是否定的。我们知道磁盘IO相比于内存读写，速度完全不是一个量级的，对于大型互联网企业应用的mysql数据库，数据库的频繁读写是少不了的，如果每条语句都直接写入磁盘，那即使做了再多的读写分离，每个数据库实例所能承载的并发连接也扛不住，用户体验自然也极差。</p>
<p>所以说，当用户插入一条数据时，是先写入缓冲池中的某几个页(因为可能不止写数据，还可能写了二级索引，而且还会向undo页等页中写入一些信息)，然后等到合适的时机，再由内部的工作线程将这些修改后的页(称为脏页)刷新到磁盘。不过并不是每次有脏页就立即被刷新到磁盘，而是基于一个checkpoint机制在合适的时机刷新到磁盘。</p>
<p>可以为innodb设置多个缓冲池实例，以减少数据库内部资源竞争，提高并发能力。(默认为1个实例，具体设置多少合适应该要根据实际情况具体来定)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like '%innodb_buffer_pool_instances%';</span><br><span class="line">+<span class="comment">------------------------------+-------+</span></span><br><span class="line">| Variable_name                | Value |</span><br><span class="line">+<span class="comment">------------------------------+-------+</span></span><br><span class="line">| innodb_buffer_pool_instances | 1     |</span><br><span class="line">+<span class="comment">------------------------------+-------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
<h2 id="LRU-List、Free-List、Flush-List"><a href="#LRU-List、Free-List、Flush-List" class="headerlink" title="LRU List、Free List、Flush List"></a>LRU List、Free List、Flush List</h2><p>经过前面的讲述我们知道缓冲池里面存放了多种类型的page，那么这些page是如何被管理的呢？其实就是基于LRU List、Free List、Flush List这三个链表进行管理的。</p>
<p>这三个链表具体是什么东东？和缓冲池又有什么关系？前面我们对缓冲池有了一个比较感性的认知，下面从伪代码角度来理性认识下缓冲池。缓冲池是一块内存区域，每一个缓冲池实例其实是有一个struct结构体对象与之对应的，这个struct结构体对象有以下一些属性(只列举了部分)</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// buf_pool_struct结构体</span></span><br><span class="line"></span><br><span class="line">frame_mem   <span class="comment">//缓冲池内存起始地址</span></span><br><span class="line">high_end    <span class="comment">//缓冲池内存终止地址</span></span><br><span class="line">blocks      <span class="comment">//这是一个数组对象，里面存放的是一个个的page!  每一个page也对应一个结构体对象buf_block_struct</span></span><br><span class="line">flush_list  <span class="comment">//指向了Flush List这个链表的首地址，也就是指向的是一个buf_block_struct，下面两个链表同理！</span></span><br><span class="line"><span class="built_in">free</span>        <span class="comment">//指向了Free List这个链表的首地址！</span></span><br><span class="line">LRU         <span class="comment">//指向了LRU List这个链表的首地址！</span></span><br><span class="line">LRU_old     <span class="comment">//指向了LRU List这个链表的old端的首地址，也是指向一个buf_block_struct对象</span></span><br><span class="line">LRU_old_len <span class="comment">//old端的page数量</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>通过上面这个结构体，我们对缓冲池这个对象结构有了比较理性的认识，也知道了LRU List、Free List、Flush List这三个链表其实就是缓冲池对象的3个属性，缓冲池中几乎所有的page都在这三个链表中(有些page不在，等讲到的时候再说，这里可以暂时理解为几乎所有的页都被这三个链表贯穿起来组成了缓冲池)。</p>
<p>Free List表示所有缓冲池中空闲的页，LRU List表示所有缓冲池中已使用的页，Flush List表示所有缓冲池中的脏页(也即等待被刷新到磁盘上的页)。需要注意的是，脏页即在Flush List，也在LRU List中(也即两个链表中同时记录了脏页buf_block_struct对象的指针)。三者的关系如下图所示：<br><img src="/images/mysql/list.png" alt=""> </p>
<p>我们可以使用mysql提供的命令查看这三个链表的一些信息:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show engine innodb status\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">  Type: InnoDB</span><br><span class="line">  Name: </span><br><span class="line">Status: </span><br><span class="line"><span class="comment">-- 中间省略了很多信息</span></span><br><span class="line"><span class="keyword">INSERT</span> BUFFER <span class="keyword">AND</span> ADAPTIVE <span class="keyword">HASH</span> <span class="keyword">INDEX</span></span><br><span class="line"><span class="comment">-------------------------------------</span></span><br><span class="line">Ibuf: <span class="keyword">size</span> <span class="number">1</span>, free <span class="keyword">list</span> <span class="keyword">len</span> <span class="number">3</span>, seg <span class="keyword">size</span> <span class="number">5</span>, <span class="number">0</span> merges</span><br><span class="line">merged <span class="keyword">operations</span>:</span><br><span class="line">...</span><br><span class="line"><span class="keyword">Hash</span> <span class="keyword">table</span> <span class="keyword">size</span> <span class="number">276671</span>, node <span class="keyword">heap</span> has <span class="number">7</span> buffer(s)</span><br><span class="line"><span class="comment">--------------------------------------------------</span></span><br><span class="line">BUFFER POOL <span class="keyword">AND</span> <span class="keyword">MEMORY</span></span><br><span class="line"><span class="comment">----------------------</span></span><br><span class="line">Total <span class="keyword">memory</span> allocated <span class="number">137363456</span>; in additional pool allocated 0</span><br><span class="line">Dictionary memory allocated 4421041</span><br><span class="line">Buffer pool size   8191</span><br><span class="line">Free buffers       3749</span><br><span class="line">Database pages     4435</span><br><span class="line">Old database pages 1657</span><br><span class="line">Modified db pages  0</span><br><span class="line">Pending reads 0</span><br><span class="line">Pending writes: LRU 0, <span class="keyword">flush</span> <span class="keyword">list</span> <span class="number">0</span>, single page <span class="number">0</span></span><br><span class="line">Pages made young <span class="number">0</span>, <span class="keyword">not</span> young <span class="number">0</span></span><br><span class="line"><span class="number">0.00</span> youngs/s, <span class="number">0.00</span> non-youngs/s</span><br><span class="line">Pages <span class="keyword">read</span> <span class="number">4435</span>, created <span class="number">0</span>, written <span class="number">2</span></span><br><span class="line"><span class="number">0.00</span> <span class="keyword">reads</span>/s, <span class="number">0.00</span> creates/s, <span class="number">0.00</span> writes/s</span><br><span class="line"><span class="keyword">No</span> buffer pool page gets since the <span class="keyword">last</span> printout</span><br><span class="line">Pages <span class="keyword">read</span> ahead <span class="number">0.00</span>/s, evicted <span class="keyword">without</span> <span class="keyword">access</span> <span class="number">0.00</span>/s, Random <span class="keyword">read</span> ahead <span class="number">0.00</span>/s</span><br><span class="line">LRU <span class="keyword">len</span>: <span class="number">4435</span>, unzip_LRU <span class="keyword">len</span>: <span class="number">0</span></span><br><span class="line">I/O <span class="keyword">sum</span>[<span class="number">0</span>]:cur[<span class="number">0</span>], unzip <span class="keyword">sum</span>[<span class="number">0</span>]:cur[<span class="number">0</span>]</span><br><span class="line"><span class="comment">--------------</span></span><br><span class="line">============================</span><br><span class="line"></span><br><span class="line"><span class="number">1</span> <span class="keyword">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></p>
<p>可以看到缓冲池一共有Buffer pool size   8191个page，其中有Free buffers       3749,LRU len: 4435。我们来做一个算数运算，Free+LRU=8184。并不等于缓冲池大小8191.还差了7个page。</p>
<p>我们刚才说到，并不是所有的page都在free list和lru list中，比如hash索引页和insert buffer页就不归这三个链表管理(但这两种页依然是从free list申请，不过申请后不归lru管理)，我们发现<code>Hash table size 276671, node heap has 7 buffer(s)</code>，也就是剩下的7个page刚好是hash索引页。</p>
<p>熟悉链表数据结构的朋友应该都知道，链表中每个节点直接内存地址并不一定是连续的，由此对于缓冲池的内存模型更具体的分布图大致如下图所示：<br><img src="/images/mysql/buffer pool.png" alt=""> </p>
<p>图中每一个方块对应一个page，这么多page通过free list和lru list两个链表贯穿起来，再加上insert buffer和lock info page和hash index page，这么多page就组成了一个缓冲池实例。</p>
<p>到现在为止，我们对缓冲池应该理解的比较透彻清晰了，但是本节的主题LRU List到底是个什么东东还没讲到，下面就重点讲这三个链表。</p>
<p>当用户执行sql查询数据库时，如果缓冲池中已经有该页的信息，那么就直接从该页读取；如果缓冲池中没有，那么就需要从磁盘读取对应位置的page(根据表空间id以及表空间内的偏移量offet定位到具体磁盘文件中的位置)并加载到缓冲池。但是，缓冲池那么大，新加在的page放到哪呢？</p>
<p>innodb内部大致工作流程如下，当发现缓冲池中没有用户所需的page时，调用内部某函数从磁盘对应位置加载相应的page到缓冲池中，此时就需要从free list中申请对应数量的page，如果free list有足够的page，就将新申请到的page添加到LRU List中。</p>
<p>但是需要注意，LRU List这个链表分为两个部分，old区域和new区域，一般来说，new区域存放的都是热点页(也就是被频繁访问的page)，old区存放的是访问不频繁的页。LRU List从起始位置一直到某个临界点，都是new区域，后面的都是old区域。这个临界点也叫midpoint位置，如下图所示：<br><img src="/images/mysql/midpoint.png" alt=""> </p>
<p>回过头看缓冲池结构体对象，发现有两个属性<code>LRU_old</code>和<code>LRU_old_len</code>，这两个属性就将LRU List划分成了2个区域。</p>
<p>新读取的page，并不是直接放在LRU链表的首部，而是放在Midpoint位置。这个位置默认为整个LRU链表的5/8，也就是说默认情况下young区域占5/8，old区域占3/8。这个参数是用户可以调整的，如下所示：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like '%innodb_old_blocks_pct%';</span><br><span class="line">+<span class="comment">-----------------------+-------+</span></span><br><span class="line">| Variable_name         | Value |</span><br><span class="line">+<span class="comment">-----------------------+-------+</span></span><br><span class="line">| innodb_old_blocks_pct | 37    |</span><br><span class="line">+<span class="comment">-----------------------+-------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></p>
<p>可以看到默认情况下，old_blocks所占百分比为37%，也即3/8。如果觉得我们的热区数据不止63%的话，可以将old的比例调低以提升性能。</p>
<p>也就是说，新插入的page是放在LRU链表的LRU_old属性后面的，大概位置在距离LRU首部节点5/8位置。那么这样做的目的是什么呢？为什么不直接将新读取到的page放在LRU链表首部？</p>
<p>举个例子，很多时候用户执行一条查询，所查询到的结果很可能是多个page，此时free链表很可能已经被用光了，那么就必须删除一些LRU末尾的页来给新页腾出位置，如果用户此次查询需要的page非常多，如果直接放到LRU首部，就很可能将真正的热区数据page从LRU删掉，而新加入的page有可能只是此次查询才会用到。这样的话，正在的热区数据被冷数据淘汰，显然会影响性能，所以才将新加入的页插入到midpoint位置。</p>
<p>当新加入的页再次被访问时，就会调用buf_LRU_make_block_young函数将其移动到链表的头部。不过需要注意的是，在LRU尾部被淘汰的页必须满足以下条件：页不是脏的、页没有正在被其他线程使用。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Database pages     4435     <span class="comment">--LRU链表总长度</span></span><br><span class="line">Old database pages 1657     <span class="comment">--Old区域长度</span></span><br><span class="line">Pages made young 0          <span class="comment">--该项表示page被移动到LRU首部的次数</span></span><br></pre></td></tr></table></figure></p>
<p>从上述信息来看，old长度/总LRU长度(1657/443=0.37)刚好等于37%。</p>
<h2 id="checkpoint机制"><a href="#checkpoint机制" class="headerlink" title="checkpoint机制"></a>checkpoint机制</h2><p>我们先来讲清楚为什么有checkpoint机制以及这个机制是做什么的。</p>
<p>此前我们讲到，当用户插入数据时，必然是将数据先插入缓冲池，然后在找一个合适的时机同步到磁盘中进行持久化。那么这个时机就很关键，如果过于频繁的同步，显然会影响性能。</p>
<p>另外还有一个问题，如果在还未同步到磁盘或者正在同步的过程中数据库宕机，那么就会产生数据丢失，所以innodb采用了write ahead log的方式。也就是当事务提交时，先写redo log，再修改page。而写redo log可以认为是原子性操作(因为是基于扇区写入)，所以即使写入过程数据库宕机，只要redo log写入成功，当数据库再次启动时就可以重放redo log进行数据恢复。</p>
<p>既然有了redo log是不是可以不将数据从缓冲池中同步到磁盘呢？所有page都在缓冲池中性能岂不是非常高？问题在于，随着数据量增大，缓冲池几乎不可能将所有数据全部缓存，另外如果一直不同步，那么redo log就会无线增大，所以显然是不可取的。另外就是，即使内存够大，redo log文件大小无限制，但如果数据库宕机再次启动时，从redo log进行数据恢复岂不是要非常久？这对于生产环境下就和灾难一样。</p>
<p>正是因为上述的一些原因，才有了checkpoint机制。由此可见checkpoint机制主要解决以下问题：</p>
<ol>
<li>缩短数据库恢复时间</li>
<li>缓冲池不够用时，将脏页刷到磁盘</li>
<li>redo log不可用时，将脏页刷到磁盘</li>
</ol>
<p>关于第一点，数据库宕机恢复时，并不需要重放所有的日志，只需要重放checkpoint后的日志，这样自然极大缩短了恢复时间。</p>
<p>关于第二点，如果缓冲池不够用(也即是free list耗尽)，需要删除LRU尾端最少使用的页，如果此页为脏页，则会强制进行checkpoint将脏页刷到磁盘。</p>
<p>关于第三点，redo log文件并不是只有1个，而且并不是无线增大，而是当redo log大到一定程度的时候，新的log内容要覆盖老内容，那么再次之前就必须强制进行checkpoint，将缓冲池中脏页至少刷新到当前redo log的位置。</p>
<p>对于innodb来言，内部有LSN进行标识，每个page有LSN，redo log也有LSN，checkpoint也有LSN，可以通过下面命令查看他们的LSN。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show engine innodb status\G;</span><br><span class="line"><span class="comment">-- 其他无关信息省略...</span></span><br><span class="line">LOG</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">Log sequence number 2576750188</span><br><span class="line">Log flushed up to   2576750188</span><br><span class="line">Last checkpoint at  2576750188</span><br><span class="line">0 pending log writes, 0 pending chkp writes</span><br><span class="line">14 log i/o's done, 0.00 log i/o's/second</span><br><span class="line"><span class="comment">----------------------</span></span><br></pre></td></tr></table></figure></p>
<p>innodb有两种checkpoint，sharp checkpoint和fuzzy checkpoint。</p>
<p>当数据库关闭时会执行sharp checkpoint将所有脏页刷新到磁盘，而在innodb运行过程中是进行fuzzy checkpoint每次只刷新部分脏页到磁盘。下面讲述有哪些情况会导致fuzzy checkpoint。</p>
<ol>
<li>Main Thread会每秒及每10秒异步的从flush list中刷新一部分脏页到磁盘</li>
<li><p>LRU list需要保证有一定量的page可用，从mysql5.6版本以后，若lru列表中没有足够的可用页，则会移除最少使用页，如果要移除的是脏页，则需要进行fuzzy checkpoint。可用页的数量默认为1024，可以通过如下命令查看：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like '%innodb_lru_scan_depth%';</span><br><span class="line">+<span class="comment">-----------------------+-------+</span></span><br><span class="line">| Variable_name         | Value |</span><br><span class="line">+<span class="comment">-----------------------+-------+</span></span><br><span class="line">| innodb_lru_scan_depth | 1024  |</span><br><span class="line">+<span class="comment">-----------------------+-------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
<p> 检查lru是否有足够的page以及移除这些页是page clean thread进行的，不会阻塞用户主线程查询操作。</p>
</li>
<li>当redo log不可用时也会发生fuzzy checkpoint。为了简单理解，我们可以认为不可用是指redo log的lsn已经比checkpoint的lsn大了很多了，导致redo log文件过大，需要进行fuzzy checkpoint来使checkpoint的lsn追上redo log，从而可以使原有的redo log内容被覆盖。具体细节大家可以参考<code>Mysql技术内幕：innodb存储引擎(第2版)</code>这本书。</li>
<li>最后一种fuzzy checkpoint是因为缓冲池中脏页太多导致强制进行fuzzy checkpoint刷新脏页到磁盘，那么怎么才算脏页过多呢？innodb提供了对应参数供用户设置，默认为75%<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like '%innodb_max_dirty_pages_pct';</span><br><span class="line">+<span class="comment">----------------------------+-----------+</span></span><br><span class="line">| Variable_name              | Value     |</span><br><span class="line">+<span class="comment">----------------------------+-----------+</span></span><br><span class="line">| innodb_max_dirty_pages_pct | 75.000000 |</span><br><span class="line">+<span class="comment">----------------------------+-----------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="innodb的Main-Thread"><a href="#innodb的Main-Thread" class="headerlink" title="innodb的Main Thread"></a>innodb的Main Thread</h1><p>innodb内部工作线程有很多，我个人觉得如果不是专业做DBA或者数据库内核开发的，普通数据库应用人员应该不需要知根知底的了解透彻每一个线程具体是做什么的以及具体有哪些线程，更何况不同的innodb版本里面线程种类、数量及负责的工作职责可能都不太一样，我们只需要理解个大概就足够了。</p>
<p>对于innodb来说，最重要的就是主线程。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show engine innodb status\G;</span><br><span class="line"><span class="comment">-- 中间省略了很多无关信息</span></span><br><span class="line">BACKGROUND THREAD</span><br><span class="line"><span class="comment">-----------------</span></span><br><span class="line">srv_master_thread loops: 323 1_second, 323 sleeps, 29 10_second, 33 background, 33 <span class="keyword">flush</span></span><br><span class="line">srv_master_thread <span class="keyword">log</span> <span class="keyword">flush</span> <span class="keyword">and</span> writes: <span class="number">323</span></span><br><span class="line"><span class="comment">----------</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，innodb在这段时间内，执行了323此每隔1秒的操作，每隔10秒的操作执行了29次，background和flush各执行了33次。</p>
<p>那么什么是每隔1秒的操作以及每隔10秒的操作呢？其实这就是master thread的工作内容，分别是每1秒一次的循环，以及每10秒一次的循环。</p>
<p>这里就以每10秒一次的操作为例来说明把，以便对于后面博客其他内容有个感性认知，以及对本篇博客的一个更加全面的补充。</p>
<p>master线程每隔10秒会做如下事情：</p>
<ol>
<li><p>刷新一定数量(因版本而不同，大概是100-200多个，而且用户可调整)的脏页到磁盘。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like '%innodb_io_capacity%';</span><br><span class="line">+<span class="comment">--------------------+-------+</span></span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+<span class="comment">--------------------+-------+</span></span><br><span class="line">| innodb_io_capacity | 200   |</span><br><span class="line">+<span class="comment">--------------------+-------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>合并一定数量的插入缓冲(具体什么是插入缓冲以及如何合并，在后面博客中详细讲解)，这个数量一般是innodb_io_capacity的5%</p>
</li>
<li>将日志缓冲刷新到磁盘</li>
<li>删除一定数量的无用undo page，这个数量默认是20个，而且可以调整，如下所示：<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like '%innodb_purge_batch_size%';</span><br><span class="line">+<span class="comment">-------------------------+-------+</span></span><br><span class="line">| Variable_name           | Value |</span><br><span class="line">+<span class="comment">-------------------------+-------+</span></span><br><span class="line">| innodb_purge_batch_size | 20    |</span><br><span class="line">+<span class="comment">-------------------------+-------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>这是innodb系列博客的第一篇，主要讲了innodb的体系结构及内存模型，这一篇博客中的内容对于后续的博客至关重要，必须要深刻理解缓冲池、LRU、page等概念才能更好的理解后续内容。</p>
<p>后面的博客文章中，将会陆续展开各个模块的内容，比如关于日志文件(undo log、redo log、binlog)、表结构和表空间及page、索引、插入缓冲及Innodb其他特性、锁与事务。</p>
<p>这些模块各个都是innodb的核心，理解这些概念对日常mysql开发至关重要。当然对于mysql来说还有很多其他重要的内容，比如分区、分表分库、主从复制、读写分离、热备份及数据迁移以及sql调优和性能监控，这些内容相对来说更偏向运维一些(sql调优和性能优化对于运维和开发人员来说同样重要)，总之…程序员的路远得很，这还只是一个mysql数据库模块…哎慢慢搞把~</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/18/java/基于NIO实现客户端与服务端聊天程序/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/18/java/基于NIO实现客户端与服务端聊天程序/" itemprop="url">基于NIO实现客户端与服务端聊天程序</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-18T00:00:00+08:00">
                2017-10-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index">
                    <span itemprop="name">java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="服务端实现"><a href="#服务端实现" class="headerlink" title="服务端实现"></a>服务端实现</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NioSocketServer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Selector selector;</span><br><span class="line">    <span class="keyword">private</span> ServerSocketChannel server;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> port = <span class="number">8080</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> TIME_OUT = <span class="number">3000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NioSocketServer</span><span class="params">(<span class="keyword">int</span> port)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.port = port;</span><br><span class="line">        server = ServerSocketChannel.open();</span><br><span class="line">        server.socket().bind(<span class="keyword">new</span> InetSocketAddress(port));</span><br><span class="line">        server.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">        selector = Selector.open();</span><br><span class="line">        server.register(selector, SelectionKey.OP_ACCEPT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ServerStart</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 如果没有可用事件，selector.select(TIME_OUT)会阻塞TIME_OUT毫秒然后返回0，如果有可用事件则直接返回事件数量</span></span><br><span class="line">            <span class="keyword">int</span> evCount = selector.select(TIME_OUT);</span><br><span class="line">            <span class="keyword">if</span> (evCount == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">// 所有channel向selector注册的事件都会在这里处理</span></span><br><span class="line">            Set&lt;SelectionKey&gt; keys = selector.selectedKeys();</span><br><span class="line">            Iterator&lt;SelectionKey&gt; it = keys.iterator();</span><br><span class="line">            <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">                SelectionKey key = it.next();</span><br><span class="line">                it.remove();</span><br><span class="line">                <span class="keyword">if</span> (key.isAcceptable()) &#123;</span><br><span class="line">                    handleAccept(key);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.isReadable()) &#123;</span><br><span class="line">                    handleRead(key);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.isWritable()) &#123;</span><br><span class="line">                    handleWrite(key);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleAccept</span><span class="params">(SelectionKey key)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 由于是isAcceptable类型的事件，所以key.channel得到的是ServerSocketChannel类型的channel，其实就是server</span></span><br><span class="line">        ServerSocketChannel ssc = (ServerSocketChannel) key.channel();</span><br><span class="line">        SocketChannel client = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            client = ssc.accept();</span><br><span class="line">            client.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">            ByteBuffer buf = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line">            System.out.println(client.socket().getRemoteSocketAddress() + <span class="string">"已连接..."</span>);</span><br><span class="line">            <span class="comment">// 当有客户端建立好连接后，向客户端发送欢迎信息</span></span><br><span class="line">            String welcome = client.socket().getRemoteSocketAddress() + <span class="string">"，你好，欢迎你！\n"</span>;</span><br><span class="line">            buf.put(welcome.getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">            buf.flip();</span><br><span class="line">            <span class="keyword">while</span> (buf.hasRemaining()) &#123;</span><br><span class="line">                client.write(buf);</span><br><span class="line">            &#125;</span><br><span class="line">            buf.clear();</span><br><span class="line">            <span class="comment">//发送完信息后，将这个channel向selector注册一个读事件</span></span><br><span class="line">            <span class="comment">//当这个channel对应的客户端发来数据并且数据可读时，就会selector就会收到通知并放入selector.selectedKeys()中</span></span><br><span class="line">            client.register(selector, SelectionKey.OP_READ, buf);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (client != <span class="keyword">null</span>) client.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e1) &#123;</span><br><span class="line">                e1.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleRead</span><span class="params">(SelectionKey key)</span> </span>&#123;</span><br><span class="line">        SocketChannel clientChannel = (SocketChannel) key.channel();</span><br><span class="line">        ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//　服务端打印收到的客户端发送来的数据</span></span><br><span class="line">            <span class="keyword">int</span> len = clientChannel.read(buf);</span><br><span class="line">            <span class="keyword">if</span> (len == -<span class="number">1</span> || len == <span class="number">0</span>) &#123;</span><br><span class="line">                clientChannel.close();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String recMsg = <span class="keyword">new</span> String(buf.array(), <span class="number">0</span>, len);</span><br><span class="line">                System.out.println(clientChannel.socket().getRemoteSocketAddress() + <span class="string">"的数据:"</span> + recMsg);</span><br><span class="line">                buf.clear();</span><br><span class="line">                <span class="comment">// 打印完客户端发送来的信息之后，服务端想要给客户端回应</span></span><br><span class="line">                <span class="comment">//所以向buf中写入了一些信息，并注册OP_WRITE事件，但是注意此时并没有真正向客户端写(虽然此时可以写)</span></span><br><span class="line">                <span class="comment">//而是等待系统告知selector当前这个clientChannel以及可写后才真正去写</span></span><br><span class="line">                <span class="comment">//另外，当服务端向客户端写完之后，一定要取消注册OP_WRITE事件，因为每个channel一直都是可写状态</span></span><br><span class="line">                <span class="comment">//也就是说key.isWritable()返回的一直都是true，所以如果不取消OP_WRITE事件的话服务端就会一直向客户端发送数据</span></span><br><span class="line">                buf.put((<span class="string">"服务端已收到您发送的数据:"</span> + recMsg).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">                clientChannel.register(key.selector(), SelectionKey.OP_WRITE, buf);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (clientChannel != <span class="keyword">null</span>) clientChannel.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e1) &#123;</span><br><span class="line">                e1.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleWrite</span><span class="params">(SelectionKey key)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//当channel可写时，用客户端发送数据</span></span><br><span class="line">        SocketChannel sc = (SocketChannel) key.channel();</span><br><span class="line">        ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">        buf.flip();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (buf.hasRemaining()) &#123;</span><br><span class="line">                sc.write(buf);</span><br><span class="line">            &#125;</span><br><span class="line">            buf.clear();</span><br><span class="line">            <span class="comment">// 写完数据之后，将OP_WRITE事件取消，并且注册OP_READ事件</span></span><br><span class="line">            sc.register(key.selector(), SelectionKey.OP_READ, buf);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (sc != <span class="keyword">null</span>) sc.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e1) &#123;</span><br><span class="line">                e1.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> NioSocketServer(<span class="number">8555</span>).ServerStart();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="客户端实现"><a href="#客户端实现" class="headerlink" title="客户端实现"></a>客户端实现</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NioSocketClient</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> SocketChannel client;</span><br><span class="line">    <span class="keyword">private</span> Selector selector;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NioSocketClient</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            client = SocketChannel.open();</span><br><span class="line">            client.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">            <span class="comment">//在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用finishConnect()的方法</span></span><br><span class="line">            client.connect(<span class="keyword">new</span> InetSocketAddress(<span class="string">"127.0.0.1"</span>, <span class="number">8555</span>));</span><br><span class="line">            selector = Selector.open();</span><br><span class="line">            client.register(selector, SelectionKey.OP_CONNECT);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                selector.close();</span><br><span class="line">                client.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e1) &#123;</span><br><span class="line">                e1.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (client.isConnectionPending()) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span>(client.finishConnect())&#123;</span><br><span class="line">                    client.register(selector, SelectionKey.OP_READ, ByteBuffer.allocate(<span class="number">1024</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    selector.close();</span><br><span class="line">                    client.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e1) &#123;</span><br><span class="line">                    e1.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">while</span> (scanner.hasNextLine()) &#123;</span><br><span class="line">            String msg = scanner.nextLine();</span><br><span class="line">            <span class="keyword">if</span> (<span class="string">""</span>.equals(msg)) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"exit"</span>.equals(msg)) System.exit(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">else</span> handle(msg);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> wait = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (wait) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">int</span> count = selector.select(<span class="number">3000</span>);</span><br><span class="line">                <span class="keyword">if</span> (count == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">                Set&lt;SelectionKey&gt; keys = selector.selectedKeys();</span><br><span class="line">                Iterator&lt;SelectionKey&gt; it = keys.iterator();</span><br><span class="line">                <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">                    SelectionKey key = it.next();</span><br><span class="line">                    it.remove();</span><br><span class="line">                    <span class="keyword">if</span> (key.isReadable()) &#123;</span><br><span class="line">                        ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">                        SocketChannel channel = (SocketChannel) key.channel();</span><br><span class="line">                        buf.clear();</span><br><span class="line">                        channel.read(buf);</span><br><span class="line">                        buf.flip();</span><br><span class="line">                        <span class="keyword">byte</span>[] by = <span class="keyword">new</span> <span class="keyword">byte</span>[buf.remaining()];</span><br><span class="line">                        buf.get(by);</span><br><span class="line">                        System.out.println(<span class="string">"接收到服务端数据:"</span> + channel.socket().getRemoteSocketAddress() + <span class="keyword">new</span> String(by, <span class="string">"UTF-8"</span>));</span><br><span class="line">                        channel.register(selector, SelectionKey.OP_WRITE, buf);</span><br><span class="line">                        wait = <span class="keyword">false</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.isWritable()) &#123;</span><br><span class="line">                        ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">                        SocketChannel channel = (SocketChannel) key.channel();</span><br><span class="line">                        buf.clear();</span><br><span class="line">                        buf.put(<span class="keyword">new</span> String(msg).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">                        buf.flip();</span><br><span class="line">                        <span class="keyword">while</span> (buf.hasRemaining()) &#123;</span><br><span class="line">                            channel.write(buf);</span><br><span class="line">                        &#125;</span><br><span class="line">                        buf.compact();</span><br><span class="line">                        channel.register(selector, SelectionKey.OP_READ, buf);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    selector.close();</span><br><span class="line">                    client.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e1) &#123;</span><br><span class="line">                    e1.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> NioSocketClient().connect();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/17/java/nio的ByteBuffer、Channel和Selector详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/17/java/nio的ByteBuffer、Channel和Selector详解/" itemprop="url">nio的ByteBuffer、Channel和Selector详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-17T00:00:00+08:00">
                2017-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index">
                    <span itemprop="name">java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NIO概述"><a href="#NIO概述" class="headerlink" title="NIO概述"></a>NIO概述</h1><p>传统IO是阻塞的，比如当服务端执行<code>serverSocket.accept()</code>的时候，就会一直阻塞在这里，当有客户端连接进来时，阻塞才会被唤醒，然后为新进来的连接启动一个线程去处理单独的客户端请求。</p>
<p>而且传统IO是基于socket的输入输出流的，比如当服务端调用<code>socket().getInputStream().read()</code>读取客户端数据时，如果客户端迟迟不发送数据(比如因为网络延迟等等原因)，服务端线程就会一直阻塞在这里。</p>
<p>当客户端连接非常多的时候，服务端就开启了大量线程，而每一个线程在jvm中都要占用不少内存，并且服务端CPU在各个线程中切换时，又会耗费很多性能，所以传统的IO方式对于客户端数量非常多的情形就不太适用。于是jdk中就引入了NIO的实现弥补传统IO在某些应用场景的不足。</p>
<p>我觉得NIO是对传统IO的补充，并不能取代传统IO，传统IO在用户量小并且单个连接传输数据量较大的时候比NIO更有优势。</p>
<p>关于NIO有几个核心概念，Buffer、Channel、Selector。</p>
<h1 id="ByteBuffer详解"><a href="#ByteBuffer详解" class="headerlink" title="ByteBuffer详解"></a>ByteBuffer详解</h1><p>相对于传统IO是基于输入输出流的来言，NIO是基于Buffer的。NIO中Buffer的具体的实现有很多，比如ByteBuffer、LongBuffer、IntBuffer等，这里只讲最重要的ByteBuffer。</p>
<p>ByteBuffer内部有几个指针非常重要，读写数据全靠这些指针来标识。如下图所示：<br><img src="/images/hadoop/buffers-modes.png" alt=""> </p>
<h2 id="capacity"><a href="#capacity" class="headerlink" title="capacity"></a>capacity</h2><p>对于ByteBuffer来说，capacity相当于其容量，最多只能向其中写入capacity个byte，如果写满之后，就再也写不进去了。</p>
<h2 id="position"><a href="#position" class="headerlink" title="position"></a>position</h2><p>初始的position值为0，当用户写入一个byte时，position就变为了1，position最大为capacity-1。不仅向ByteBuffer中写数据会改变position，从其中读数据也会改变position。</p>
<h2 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h2><p>limit像是一堵墙卡在那里，标识着用户最多能读到limit位置或者最多能写多少字节。</p>
<p>想用好ByteBuffer就必须要深刻理解上述的3个标识变量，因为ByteBuffer的很多个API方法都是在操作这3个变量，ByteBuffer就是根据这3个指针来控制读写。</p>
<p>下面就介绍一些ByteBuffer中非常常用的方法：</p>
<p>首先是生成ByteBuffer。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 生成一个capacity为48的buf</span></span><br><span class="line">ByteBuffer buf = ByteBuffer.allocate(<span class="number">48</span>);</span><br><span class="line"><span class="comment">// 也可以根据已有的byte[]来创建，这样的话，就将b这个数组“绑定”到了buf上，需要注意的是，当数组b内容变化时，buf中数据也会变化</span></span><br><span class="line"><span class="keyword">byte</span>[] b = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">48</span>];</span><br><span class="line">ByteBuffer buf = ByteBuffer.warp(b);</span><br></pre></td></tr></table></figure></p>
<p>然后是ByteBuffer的一些读写常用的方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 一般情况下，在向ByteBuffer中写入数据之前都要先clear下，将buf“清空”，其实可以看到数据并没有清空，只是改变了内部的指针位置</span></span><br><span class="line"><span class="comment">// 所以可以将clear方法看做是将Buf切换到写模式</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    position = <span class="number">0</span>;</span><br><span class="line">    limit = capacity;</span><br><span class="line">    mark = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 当buf中写完数据后，准备读数据之前，需要调用flip方法来切换到读模式</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">flip</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    limit = position;</span><br><span class="line">    position = <span class="number">0</span>;</span><br><span class="line">    mark = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// rewind方法只是简单将position = 0，有时候也会用到此方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">rewind</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    position = <span class="number">0</span>;</span><br><span class="line">    mark = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 这个方法可以用来获取buf中未读的数据的长度，也很常用，比如根据bytebuffer的长度创建一个byte[] b = new byte[buf.remaining()]</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">remaining</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> limit - position;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个方法经常用来判断buf中是否还有数据未读取完</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">hasRemaining</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> position &lt; limit;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// compact方法相对来说是这些方法中最难理解的一个，下面重点说一下</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">compact</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.arraycopy(hb, ix(position()), hb, ix(<span class="number">0</span>), remaining());</span><br><span class="line">    position(remaining());</span><br><span class="line">    limit(capacity());</span><br><span class="line">    discardMark();</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>从上述代码中可以看到，compact方法是将从position位置，一直到limit-1位置的这些字节数据，copy到0的位置，然后讲position = limit - position，并将limit置为capacity。</p>
<p>也就是说，如果remaining()为0的情况下，compact的效果几乎等同于clear。下图可以很清晰的展示compact方法：</p>
<p>执行compact之前:<br><img src="/images/hadoop/compact1.jpg" alt=""> </p>
<p>compact后<br><img src="/images/hadoop/compact2.jpg" alt=""> </p>
<p>一般什么时候会用到compact方法呢？一般是在write的时候，因为很多时候write方法并不能一次性将ByteBuffer中的数据读取完，如果直接clear的话，那么当新数据写入时，之前未读完的数据就会被覆盖。所以此时可以在write之后调用compact，将未读完的数据copy的buffer的0位置处，然后新数据就会在老数据后面的位置开始写，之前的数据就不会被覆盖。</p>
<h1 id="Channel详解"><a href="#Channel详解" class="headerlink" title="Channel详解"></a>Channel详解</h1><p>Channel就好像传统IO中的输入输出流，只不过传统IO的输入输出流是按字节或字符读取或写入数据，而Channel是操作的ByteBuffer。</p>
<p>Channel的实现有FileChannel、SocketChannel、ServerSocketChannel、DatagramChannel。其中，FileChannel可以从文件中读写数据，SocketChannel能通过TCP读写数据，ServerSocketChannel可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。</p>
<p>下面通过一个copy文件的例子来更好的理解和运用Channel和ByteBuffer。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">CopyFile</span><span class="params">(String srcPath, String dstPath)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     <span class="comment">//建立file的输入输出流</span></span><br><span class="line">     FileInputStream infs = <span class="keyword">new</span> FileInputStream(srcPath);</span><br><span class="line">     FileOutputStream outfs = <span class="keyword">new</span> FileOutputStream(dstPath);</span><br><span class="line">     <span class="comment">//通过file流获得nio的channel</span></span><br><span class="line">     FileChannel inc = infs.getChannel();</span><br><span class="line">     FileChannel outc = outfs.getChannel();</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建一个用于nio传输数据的buffer</span></span><br><span class="line">     ByteBuffer buf = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line">     <span class="comment">//len表示从channel中读到buf的长度</span></span><br><span class="line">     <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">     <span class="keyword">while</span> ((len = inc.read(buf)) != -<span class="number">1</span>) &#123;    <span class="comment">//向buf里面写数据</span></span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">             此时buf已经写入了数据，buf内部position指针已经置于len长度位置</span></span><br><span class="line"><span class="comment">             所以如果需要将buf中的数据读出来并写入到另外一个文件中时，就需要调用buf.flip()</span></span><br><span class="line"><span class="comment">             将limit = position; position = 0</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">         buf.flip();</span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">             由于outc.write无法保证一次性将buf中的数据读取完，所以不能直接clear</span></span><br><span class="line"><span class="comment">             此处使用buf.hasRemaining()判断是否有未读取完的数据，如果有就继续读</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="keyword">while</span> (buf.hasRemaining()) &#123;</span><br><span class="line">             outc.write(buf);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">             跳出while循环后，说明buf中的数据以及完全被读取，所以可以使用clear将buf中指针置为写模式</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">         buf.clear();</span><br><span class="line">     &#125;</span><br><span class="line">     outc.close();</span><br><span class="line">     inc.close();</span><br><span class="line">     outfs.close();</span><br><span class="line">     infs.close();</span><br><span class="line">     System.out.println(<span class="string">"finished!"</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<p>上面是使用while循环+clear的方式来确保数据的读写正确，下面介绍使用compact方式来读写数据:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copy</span><span class="params">(ReadableByteChannel src, WritableByteChannel des)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        ByteBuffer buf = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line">        <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>((len = src.read(buf)) != -<span class="number">1</span>)&#123; <span class="comment">//向buf里面写数据</span></span><br><span class="line">            <span class="comment">// 写完数据后，切换到读模式开始读取buf中的数据</span></span><br><span class="line">            buf.flip();</span><br><span class="line">            des.write(buf);</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">                由于outc.write无法保证一次性将buf中的数据读取完，所以不能直接clear</span></span><br><span class="line"><span class="comment">                调用compact方法将未读完的数据copy的buf的起始位置等待数据写入</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            buf.compact();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            可能会有朋友疑问这里为什么还要在执行一次读？其实是有必要的!我们考虑这样一种情况：</span></span><br><span class="line"><span class="comment">            当上面的while循环最后一次执行src.read(buf)，将文件最后的内容写入到buf中，假设此时des.write(buf)刚好没有将buf完全读取完</span></span><br><span class="line"><span class="comment">            然后就执行了buf.compact()，由于文件已经读取完，所以再次src.read(buf)=-1，那么就不会再进入while循环</span></span><br><span class="line"><span class="comment">            那么岂不是还有一些字节还未读完吗？所以下面的读取就是为了防止这种情况发送，确保将数据一个字节不差的读取完整        </span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        buf.flip();</span><br><span class="line">        <span class="keyword">while</span>(buf.hasRemaining())&#123;</span><br><span class="line">            des.write(buf);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h1><p>在NIO中Selector就好像一个大管家，NIO之所以可以做到非阻塞，就是源于Selector。这个Selector会不断的轮训检测，当有客户端连接、有客户端数据传送过来后，Selector就会收到通知，然后我们就可以进行相关操作。</p>
<p>要想使用Selector，就必须将channel设置为非阻塞模式，所以前面介绍的FileChannel就无法使用Selector，因为FileChannel无法设置为非阻塞，而SocketChannel和ServerSocketChannel都是可以设置为非阻塞的！</p>
<p>要想使用Selector，就要先创建。为了将Channel和Selector配合使用，必须将channel注册到selector上。通过<code>SelectableChannel.register()</code>方法来实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Selector selector = Selector.open();</span><br><span class="line">channel.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">SelectionKey key = channel.register(selector,Selectionkey.OP_READ);</span><br></pre></td></tr></table></figure></p>
<p>上述代码具体有什么作用呢？首先使用Selector的open静态方法创建了Selector，然后将channel设置为非阻塞模式，然后将channel注册到selector上，并且告诉selector这个channel对OP_READ感兴趣！</p>
<p>意思就是：当这个channel上一旦有数据可读，系统就会通知正在不断轮训检测的Selector，然后我们程序员就可以写相关代码读取channel上的数据并进行相关处理。</p>
<p>一个Selector可以管理很多个channel，一个channel就好比传统IO中的一个客户端连接，用户客户端和服务端之间进行通信(具体管理多少个合适我也不是很懂= =！)，每个channel都可以向Selector注册自己感兴趣的事件，一旦这个channel上有感兴趣的事件来了，Selector就会收到通知。</p>
<p>channel向selector注册时还可以附件一些信息，比如:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ByteBuffer buf = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line">buf.put(<span class="string">"你好！"</span>.getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">client.register(selector, SelectionKey.OP_READ, buf);</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/15/spark/spark集群安装与部署/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/15/spark/spark集群安装与部署/" itemprop="url">spark集群安装与部署</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-15T00:00:00+08:00">
                2017-10-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="安装scala"><a href="#安装scala" class="headerlink" title="安装scala"></a>安装scala</h1><p>这里同样准备三台机器node0、node1、node2来部署spark集群。安装spark之前，需要先安装scala环境。</p>
<p>下载scala安装包，这里采用的是scala-2.12.4版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.scala-lang.org/files/archive/scala-2.12.4.tgz</span><br><span class="line"></span><br><span class="line">tar zxf scala-2.12.4.tgz -C /usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure>
<p>配置环境变量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala-2.12.4</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SCALA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line">scp /etc/profile node1:/etc</span><br><span class="line">scp /etc/profile node2:/etc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在三台机器上同时重读配置文件</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p>
<p>分发安装包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r scala-2.12.4/ node1:/usr/<span class="built_in">local</span>/</span><br><span class="line">scp -r scala-2.12.4/ node2:/usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure></p>
<h1 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h1><p>spark可以直接从hdfs或者hbase中读数据，所以如果需要从hdfs或hbase读取数据的话，还需要配置hadoop集群和hbase集群。三台机器上运行的进程如下:</p>
<p>node0 -&gt; namenode   resourcemanager     Master      Worker      HMaster<br>node1 -&gt; datanode   nodemanager     Worker  HRegionServer<br>node2 -&gt; datanode   nodemanager     Worker  HRegionServer</p>
<p>这里采用的是spark2.1.2，基于hadoop2.6的版本，先去官网下载安装包。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.1.2/spark-2.1.2-bin-hadoop2.6.tgz</span><br><span class="line"></span><br><span class="line">tar zxf spark-2.1.2-bin-hadoop2.6.tgz -C /usr/<span class="built_in">local</span></span><br></pre></td></tr></table></figure></p>
<p>配置环境变量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/spark-2.1.2-bin-hadoop2.6</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/sbin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line">scp /etc/profile node1:/etc</span><br><span class="line">scp /etc/profile node2:/etc</span><br><span class="line"><span class="comment"># 在三台机器上同时重读配置文件</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p>
<p>修改配置文件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/spark-2.1.2-bin-hadoop2.6/conf</span><br><span class="line">cp slaves.template slaves</span><br><span class="line"><span class="comment"># 指定哪些节点为worker</span></span><br><span class="line">vim slaves</span><br><span class="line">node0</span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line"></span><br><span class="line">cp spark-env.sh.template spark-env.sh</span><br><span class="line">vim spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=node0  <span class="comment">#指定master</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077   <span class="comment">#指定master端口</span></span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_INSTANCES=1 <span class="comment">#指定每个节点运行的worker进程数量</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_WEBUI_PORT=8080 <span class="comment">#webui界面端口</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_161/   <span class="comment">#最好指定下JAVA_HOME，不然使用start-all.sh脚本启动时读不到JAVA_HOME</span></span><br></pre></td></tr></table></figure></p>
<p>分发安装包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-2.1.2-bin-hadoop2.6/ node1:/usr/<span class="built_in">local</span>/</span><br><span class="line">scp -r spark-2.1.2-bin-hadoop2.6/ node2:/usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure></p>
<p>启动spark集群<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure></p>
<p>然后可以在各节点上使用jps查看是否启动成功，也可以在浏览器访问<a href="http://node0:8080来查看spark集群的web页面。启动spark-shell环境：" target="_blank" rel="noopener">http://node0:8080来查看spark集群的web页面。启动spark-shell环境：</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master spark://node0:7077</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/14/storm/storm系列二-storm初体验之wordcount程序编写/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/14/storm/storm系列二-storm初体验之wordcount程序编写/" itemprop="url">storm系列二:storm初体验之wordcount程序编写</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-14T00:00:00+08:00">
                2017-10-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一篇文章中我们进行了storm集群的安装部署，并启动了storm集群，这篇文章，我们写一个最简单的wordcount的示例程序。</p>
<h2 id="项目依赖"><a href="#项目依赖" class="headerlink" title="项目依赖"></a>项目依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.storm<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>storm-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Topology编写"><a href="#Topology编写" class="headerlink" title="Topology编写"></a>Topology编写</h2><p>在storm中，被提交的任务称为Topology，每一个storm任务都应该有一个Topology主驱动类，用来设置当前storm的并发度和数据执行流程等信息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopologyMain</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">        <span class="comment">// 设置数据源，一般情况下数据源有很多种，比如kafka消息队列等，这里采用比较简单的随机字符串，并设置2个并发度</span></span><br><span class="line">        builder.setSpout(<span class="string">"strings"</span>, <span class="keyword">new</span> RandomStringSpout(),<span class="number">2</span>);</span><br><span class="line">        <span class="comment">// shuffleGrouping("strings")表示从strings这个spout随机拉取数据到多个并行度的splitBolt中</span></span><br><span class="line">        builder.setBolt(<span class="string">"splitBolt"</span>, <span class="keyword">new</span> SplitBolt(),<span class="number">4</span>).shuffleGrouping(<span class="string">"strings"</span>);</span><br><span class="line">        <span class="comment">// fieldsGrouping("splitBolt",new Fields("word")) 表示按字段(hashCode)从上游bolt(splitBolt)中拉取数据</span></span><br><span class="line">        <span class="comment">// 由于是字段的hashCode，所以相同的单词必定会被传输到同一个wordCountBolt中</span></span><br><span class="line">        <span class="comment">// 每一个spout、bolt都可以emit多个字段，word表示wordCountBolt按照splitBolt的word字段从splitBolt拉取数据</span></span><br><span class="line">        builder.setBolt(<span class="string">"wordCountBolt"</span>,<span class="keyword">new</span> WorldCountBolt(),<span class="number">4</span>).fieldsGrouping(<span class="string">"splitBolt"</span>,<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">        <span class="comment">// storm程序可以在本地启动，也可以打成jar包使用 bin/storm jar xxx.jar mainclass的命令在storm集群上运行</span></span><br><span class="line">        <span class="comment">// 集群模式启动</span></span><br><span class="line">        Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">        <span class="comment">// 设置4个worker数量，一个worker可以认为就是一个jvm。</span></span><br><span class="line">        <span class="comment">// 前一篇文章中，我们部署的storm集群有2个supervisor节点，所以每个supervisor节点上面要启动2个worker进程</span></span><br><span class="line">        conf.setNumWorkers(<span class="number">4</span>);</span><br><span class="line">        StormSubmitter.submitTopologyWithProgressBar(<span class="string">"wordcount"</span>,conf,builder.createTopology());</span><br><span class="line">        <span class="comment">// 本地模式启动</span></span><br><span class="line">        <span class="comment">//conf.setMaxTaskParallelism(3);</span></span><br><span class="line">        <span class="comment">//LocalCluster cluster = new LocalCluster();</span></span><br><span class="line">        <span class="comment">//cluster.submitTopology("word-count", conf, builder.createTopology());</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="spout编写"><a href="#spout编写" class="headerlink" title="spout编写"></a>spout编写</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RandomStringSpout</span> <span class="keyword">extends</span> <span class="title">BaseRichSpout</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> SpoutOutputCollector collector;</span><br><span class="line">    <span class="keyword">private</span> Random rand;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector)</span> </span>&#123;</span><br><span class="line">        collector = spoutOutputCollector;</span><br><span class="line">        rand = <span class="keyword">new</span> Random();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String[] strs = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">          <span class="string">"hello world"</span>,<span class="string">"hello ygq hello tom"</span>,</span><br><span class="line">          <span class="string">"my name is ygq"</span>,<span class="string">"what is your name"</span></span><br><span class="line">        &#125;;</span><br><span class="line">        String str = strs[rand.nextInt(strs.length)];</span><br><span class="line">        <span class="comment">// 将随机的某个字符串发射到下游bolt中</span></span><br><span class="line">        collector.emit(<span class="keyword">new</span> Values(str));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 表示这个spout发出的数据字段叫line</span></span><br><span class="line">        outputFieldsDeclarer.declare(<span class="keyword">new</span> Fields(<span class="string">"line"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="bolt编写"><a href="#bolt编写" class="headerlink" title="bolt编写"></a>bolt编写</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> OutputCollector collector;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map map, TopologyContext topologyContext, OutputCollector outputCollector)</span> </span>&#123;</span><br><span class="line">        collector = outputCollector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 从tuple中取出spout中的line字段的数据，上游spout有可能发出了多个字段，不过本例中spout只发了一个字段line</span></span><br><span class="line">        String line = tuple.getStringByField(<span class="string">"line"</span>);</span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span>(String word : words)&#123;</span><br><span class="line">            <span class="comment">// 将切分后的单词发射到下游bolt</span></span><br><span class="line">            collector.emit(<span class="keyword">new</span> Values(word));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 表示这个bolt发出的数据字段叫word</span></span><br><span class="line">        outputFieldsDeclarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WorldCountBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> OutputCollector collector;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        这个map用来存每个单词的数量，这里用map来存储其实值得商榷，主要需要考虑几个问题：</span></span><br><span class="line"><span class="comment">            1. 有可能是多个WorldCountBolt线程在同一个worker进程内，那么这个map会不会有线程安全问题？</span></span><br><span class="line"><span class="comment">            其实，由于是根据字段分组fieldsGrouping，所以相同的word必然分配到了相同的WorldCountBolt</span></span><br><span class="line"><span class="comment">            所以在多个线程同时对这个map进行操作时，操作的其实是不同的key，所以是线程安全的</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">            2. 多个WorldCountBolt线程也有可能是运行在多个worker进程中的，甚至可能是运行在多台节点上，</span></span><br><span class="line"><span class="comment">            那么多个WorldCountBolt线程操作的map压根就不在一个进程内，压根就不是同一个map对象，</span></span><br><span class="line"><span class="comment">            所以每个WorldCountBolt线程的map的数据都不完整(但相同单词的个数是正确的)，</span></span><br><span class="line"><span class="comment">            需要将不同进程内的多个map拼接起来才是完整的结果</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">            3. 这里只是为了简单演示才使用了map这种数据结构，一般情况下，可以将计算结果存储到外部介质比如redis</span></span><br><span class="line"><span class="comment">    */</span>   </span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map map, TopologyContext topologyContext, OutputCollector outputCollector)</span> </span>&#123;</span><br><span class="line">        collector = outputCollector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        String word = tuple.getStringByField(<span class="string">"word"</span>);</span><br><span class="line">        <span class="keyword">if</span>(map.containsKey(word))&#123;</span><br><span class="line">            Integer count = map.get(word);</span><br><span class="line">            count++;</span><br><span class="line">            map.put(word,count);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            map.put(word,<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(Thread.currentThread().getName()+<span class="string">" "</span>+map);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 由于没有下游bolt了，所以声不声明输出字段都无所谓了</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/13/storm/storm系列一-storm集群安装部署/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/13/storm/storm系列一-storm集群安装部署/" itemprop="url">storm系列一:storm集群安装部署</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-13T00:00:00+08:00">
                2017-10-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参照storm官网，storm集群部署主要分为一下几步：</p>
<ol>
<li>部署zookeeper集群。storm集群重度依赖zk，用zk来记录很多信息，所以部署storm集群之前要先部署好zk集群</li>
<li>下载安装包并解压到storm集群各节点上</li>
<li>修改各节点的配置文件</li>
<li>使用storm提供的脚本启动storm集群</li>
</ol>
<h1 id="集群安装部署"><a href="#集群安装部署" class="headerlink" title="集群安装部署"></a>集群安装部署</h1><h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2><p>一共使用三台机器，分别是node0、node1、node2，这三台机器上也部署了zk服务端，使用node0作为nimbus，其他两台机器作为supervisor用来启动work。</p>
<p>这里采用的storm安装版本是apache-storm-1.2.1。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxf apache-storm-1.2.1.tar.gz -C /usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/apache-storm-1.2.1/conf</span><br><span class="line">vim storm-env.sh </span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_161/</span><br><span class="line"><span class="comment"># 将原有配置文件备份一份</span></span><br><span class="line">cp storm.yaml&#123;,.bak&#125;</span><br><span class="line">vim storm.yaml </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置zk集群</span></span><br><span class="line">storm.zookeeper.servers:</span><br><span class="line">     - <span class="string">"node0"</span></span><br><span class="line">     - <span class="string">"node1"</span></span><br><span class="line">     - <span class="string">"node2"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置nimbus节点</span></span><br><span class="line"> nimbus.seeds: [<span class="string">"node0"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nimbus和Supervisor 用来存放jar和conf等文件的目录</span></span><br><span class="line"> storm.local.dir: <span class="string">"/var/storm_data"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一个端口对应一个work进程，代表这个supervisor在本机器上最多启用多少个work，以及占用哪些端口</span></span><br><span class="line"> supervisor.slots.ports:</span><br><span class="line">    - 6700</span><br><span class="line">    - 6701</span><br><span class="line">    - 6702</span><br><span class="line">    - 6703</span><br><span class="line"></span><br><span class="line">scp -r apache-storm-1.2.1/ node1:/usr/<span class="built_in">local</span>/</span><br><span class="line">scp -r apache-storm-1.2.1/ node2:/usr/<span class="built_in">local</span>/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在每一台机器上创建数据目录</span></span><br><span class="line">mkdir -p /var/storm_data</span><br></pre></td></tr></table></figure></p>
<p>要注意storm.yaml配置文件中配置项中的空格，格式必须正确，否则可能启动失败</p>
<h2 id="启动storm集群"><a href="#启动storm集群" class="headerlink" title="启动storm集群"></a>启动storm集群</h2><p>在node0上启动Nimbus<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/storm nimbus &amp;&gt; /dev/null &amp;</span><br></pre></td></tr></table></figure></p>
<p>在node1和node2上启动supervisor<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/storm supervisor  &amp;&gt; /dev/null &amp;</span><br></pre></td></tr></table></figure></p>
<p>在node1上启动storm ui服务，可以在浏览器里查看storm集群的运行情况<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/storm ui  &amp;&gt; /dev/null &amp;</span><br></pre></td></tr></table></figure></p>
<p>然后在浏览器中输入: <a href="http://node1:8080即可查看storm集群的运行状态" target="_blank" rel="noopener">http://node1:8080即可查看storm集群的运行状态</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/11/web架构/hbase集群部署与基本使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/11/web架构/hbase集群部署与基本使用/" itemprop="url">hbase集群部署与基本使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-11T00:00:00+08:00">
                2017-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="hbase安装部署"><a href="#hbase安装部署" class="headerlink" title="hbase安装部署"></a>hbase安装部署</h1><h2 id="解压安装"><a href="#解压安装" class="headerlink" title="解压安装"></a>解压安装</h2><p>这里使用的是1.4.2版本的hbase，可以去官网下载habse安装包，下载后进行解压<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf hbase-1.4.2-bin.tar.gz -C /usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure></p>
<p>为了减少scp时间，可以将一些无用文件删掉<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hbase-1.4.2</span><br><span class="line">rm -rf docs/ *.txt</span><br></pre></td></tr></table></figure></p>
<p>hbase分为HMaster和HRegionServer，其中HMaster主要管理元数据和HRegionServer的负载均衡等协调工作，而HRegionServer是客户端真正读写数据的地方，对于hbase集群来说，需要配置文件指定哪些是HRegionServer。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim conf/regionservers</span><br><span class="line"><span class="comment"># 将默认的localhost改为如下内容：</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>这里需要说明，一共有3台机器，node0为namenode和hmaster，node1和node2为datanode和regionserver。</p>
</blockquote>
<p>然后需要配置环境变量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export  HBASE_HOME=/usr/local/hbase-1.4.2"</span> &gt;&gt; /etc/profile</span><br><span class="line"></span><br><span class="line">vim conf/vim hbase-env.sh</span><br><span class="line"><span class="comment"># 填入自己的JAVA_HOME变量</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_161/</span><br><span class="line"><span class="comment"># 不使用hbase默认自带的zookeeper，而使用自己集群配置的zk</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>修改hbase的核心配置文件hbase-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">　<span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- hbase存放数据目录 --&gt;</span></span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 指向hdfs的namenode路径--&gt;</span></span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node0:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"> <span class="comment">&lt;!-- 是否分布式部署 --&gt;</span></span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- zk地址 --&gt;</span></span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>node0,node1,node2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> 　　　</span><br><span class="line">　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--zookooper配置、日志等的存储位置 --&gt;</span></span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/hbase_zk_data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">　　<span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>分发安装包到其他节点:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r hbase-1.4.2/ node1:/usr/<span class="built_in">local</span>/</span><br><span class="line">scp -r hbase-1.4.2/ node2:/usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure></p>
<p>启动hbase集群，首先要确保hdfs集群和zk集群已启动<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-hbase.sh</span><br></pre></td></tr></table></figure></p>
<p>一定要注意的是，几个节点之间的时间一定要同步，否则master会认为一些不同步的regionserver已经挂掉而将其kill了。</p>
<h1 id="java-api操作habse"><a href="#java-api操作habse" class="headerlink" title="java api操作habse"></a>java api操作habse</h1><p>新版的hbase的api有很多变化，之前很多老版本的api已经废弃，这里采用的是hbase1.4.2版本的api，首先导入项目依赖<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="创建表和删除表"><a href="#创建表和删除表" class="headerlink" title="创建表和删除表"></a>创建表和删除表</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Admin admin;</span><br><span class="line">    <span class="keyword">private</span> Connection conn;</span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Before</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 创建config对象，主要是指定zk地址，因为zk里面记录着regionServer的地址，所以必须要连接zk，客户端才能知道去哪找regionServer</span></span><br><span class="line">        Configuration config = HBaseConfiguration.create();</span><br><span class="line">        config.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"node0,node1,node2"</span>);</span><br><span class="line">        config.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span><br><span class="line">        <span class="comment">// 通过config对象获取Connection连接对象</span></span><br><span class="line">        <span class="keyword">this</span>.conn = ConnectionFactory.createConnection(config);</span><br><span class="line">        <span class="comment">// 通过Connection对象获取Admin管理类的实例，Admin是一个接口，之前版本的HBaseAdmin实现类已经废弃</span></span><br><span class="line">        <span class="keyword">this</span>.admin = conn.getAdmin();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">CreateTable</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 创建表描述对象，表名为testTable，并向其中添加两个列族，分别是personInfo和companyInfo</span></span><br><span class="line">        HTableDescriptor htd = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(<span class="string">"testTable"</span>));</span><br><span class="line">        HColumnDescriptor hcd=<span class="keyword">new</span> HColumnDescriptor(<span class="string">"personInfo"</span>);</span><br><span class="line">        htd.addFamily(hcd);</span><br><span class="line">        HColumnDescriptor hcd2=<span class="keyword">new</span> HColumnDescriptor(<span class="string">"companyInfo"</span>);</span><br><span class="line">        htd.addFamily(hcd2);</span><br><span class="line">        <span class="comment">// 创建表</span></span><br><span class="line">        admin.createTable(htd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">DeleteTable</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 删除表之前必须先disableTable</span></span><br><span class="line">        admin.disableTable(TableName.valueOf(<span class="string">"testTable2"</span>));</span><br><span class="line">        admin.deleteTable(TableName.valueOf(<span class="string">"testTable2"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h2><p>这里只列举添加单条数据，插入批量数据和插入单条几乎一样，只不过传入的是一个List<put><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">AddData</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Table table = conn.getTable(TableName.valueOf(<span class="string">"testTable"</span>));</span><br><span class="line">    <span class="comment">// 插入单条数据</span></span><br><span class="line">    Put row =<span class="keyword">new</span> Put(Bytes.toBytes(<span class="number">1</span>)); <span class="comment">//设置rowkey</span></span><br><span class="line">    <span class="comment">//向rowkey=1　的personInfo列族添加name=ygq 和 age=27</span></span><br><span class="line">    row.addColumn(Bytes.toBytes(<span class="string">"personInfo"</span>),Bytes.toBytes(<span class="string">"name"</span>),Bytes.toBytes(<span class="string">"ygq"</span>));</span><br><span class="line">    row.addColumn(Bytes.toBytes(<span class="string">"personInfo"</span>),Bytes.toBytes(<span class="string">"age"</span>),Bytes.toBytes(<span class="number">27</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//向rowkey=1　的companyInfo列族添加name=zondy 和 tel=58767</span></span><br><span class="line">    row.addColumn(Bytes.toBytes(<span class="string">"companyInfo"</span>),Bytes.toBytes(<span class="string">"name"</span>),Bytes.toBytes(<span class="string">"zondy"</span>));</span><br><span class="line">    row.addColumn(Bytes.toBytes(<span class="string">"companyInfo"</span>),Bytes.toBytes(<span class="string">"tel"</span>),Bytes.toBytes(<span class="string">"58767"</span>));</span><br><span class="line">    table.put(row);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></put></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/10/web架构/hdfs的namenode管理元数据机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/10/web架构/hdfs的namenode管理元数据机制/" itemprop="url">hdfs的管理元数据机制和RPC框架演示</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-10T00:00:00+08:00">
                2017-10-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="namenode元数据管理"><a href="#namenode元数据管理" class="headerlink" title="namenode元数据管理"></a>namenode元数据管理</h1><p>通过前面几篇博客的讲述，我们知道对于hdfs来说，namenode是至关重要的，客户端不管是上传还是下载文件，都要去namenode中寻找具体的block在哪些datanode上，所以如果namenode的元数据信息丢失，那对于hdfs集群来说是致命的。</p>
<p>既然如此，hdfs是如何管理自己的元数据不丢失的？</p>
<p>假设，用户新上传了一个文件到hdfs，那么namenode中是不是又多了一个元数据。那么这个元数据到底存在哪里？是直接写入磁盘吗？如果是直接写入磁盘，那么如果客户端非常多，大量上传文件时，性能必然很低。就好像mysql插入数据时一样，mysql也不是直接插入磁盘的，而是会先缓存在内存中，然后找一个时机在同步到磁盘里面持久化。</p>
<p>如果namenode将元数据写入内存，新的问题又来了，如果元数据全部保存在内存中的话，虽然性能提高了，但是如果namenode宕机了呢？元数据岂不是全部丢失了，这绝对是灾难。所以namenode必然要每隔一段时间就将元数据信息写入磁盘，以保证数据安全。</p>
<p>所以在前面博客中我们看到了在namenode中有类似于<code>fsimage_0000000000000000028</code>这种文件。这种文件其实就是将namenode元数据信息的内存对象dump(序列号)到磁盘文件。但问题是多久dump一次呢？如果每来一条新的元数据就dump一次肯定不合适，如果间隔太久肯定也不合适，因为假设间隔10分钟同步一次，那么这10分钟之内如果namenode挂了，那这10分钟的数据就丢失了。</p>
<p>由于上述的种种原因，所以hdfs中另一个角色secondary namenode出场了。secondary namenode的主要职责就是管理元数据的镜像文件，他们的工作机制如下图所示：</p>
<p><img src="/images/hadoop/元数据管理.png" alt=""> </p>
<p>每当有新的元数据更新请求时，namenode会先更新管理元数据的内存对象，然后不会写入磁盘，而是写入一个edits日志文件。上篇博客中，我们介绍了namenode的元数据目录结构，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@ygq hdfsdata] tree</span><br><span class="line">.</span><br><span class="line">└── dfs</span><br><span class="line">    ├── name</span><br><span class="line">    │   ├── current</span><br><span class="line">    │   │   ├── edits_0000000000000000001-0000000000000000002</span><br><span class="line">    │   │   ├── edits_0000000000000000003-0000000000000000015</span><br><span class="line">    │   │   ├── edits_0000000000000000016-0000000000000000022</span><br><span class="line">    │   │   ├── edits_0000000000000000023-0000000000000000024</span><br><span class="line">    │   │   ├── edits_0000000000000000025-0000000000000000026</span><br><span class="line">    │   │   ├── edits_inprogress_0000000000000000027</span><br><span class="line">    │   │   ├── fsimage_0000000000000000024</span><br><span class="line">    │   │   ├── fsimage_0000000000000000024.md5</span><br><span class="line">    │   │   ├── fsimage_0000000000000000026</span><br><span class="line">    │   │   ├── fsimage_0000000000000000026.md5</span><br><span class="line">    │   │   ├── seen_txid</span><br><span class="line">    │   │   └── VERSION</span><br><span class="line">    │   └── in_use.lock</span><br><span class="line">    └── namesecondary</span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_0000000000000000001-0000000000000000002</span><br><span class="line">        │   ├── edits_0000000000000000003-0000000000000000015</span><br><span class="line">        │   ├── edits_0000000000000000016-0000000000000000022</span><br><span class="line">        │   ├── edits_0000000000000000023-0000000000000000024</span><br><span class="line">        │   ├── edits_0000000000000000025-0000000000000000026</span><br><span class="line">        │   ├── fsimage_0000000000000000024</span><br><span class="line">        │   ├── fsimage_0000000000000000024.md5</span><br><span class="line">        │   ├── fsimage_0000000000000000026</span><br><span class="line">        │   ├── fsimage_0000000000000000026.md5</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure></p>
<p>可以看到有一个个的edits文件。其中namesecondary就是secondary namenode。这里需要说明的是，一般情况下这两个目录不会在同一台服务器，这里只是由于实验环境为了节省机器，所以将secondary namenode和主namenode配置在同一台机器，正常情况下两者是分开的(HA集群下情况另算)。</p>
<p>通过上述结构，我们可以看出，edits日志文件也是可以滚动的，edits_inprogress代表正在写入的。</p>
<p>edits文件中记录的是什么呢？其实里面记录的是元数据的更新操作，是追加性写入(非常类似与mysql中的redo log)。那么如果有了edits文件，前面所提到的问题就解决了，比如假设上一次同步磁盘之后的某段时间namenode宕机，那再次启动namenode时就不会丢失数据，因为可以使用最新的fsimage+edits文件得到宕机前的所有数据。</p>
<p>但是这样有引入了新的问题，通过上面结构我们也看到了edits文件随着日积月累会越来越多，那么当namenode宕机再次启动时，岂不是要恢复很久？所以为了解决这个问题，就需要定期的将edits文件和fsimage文件合并，这样的话，namenode再次启动时，edits文件只有很小一部分需要合并，速度就会快很多。这个合并工作肯定不能归namenode自己去做，所以是有secondary namenode来进行合并的，基于此就引入了checkpoint机制。</p>
<h2 id="checkpoint机制"><a href="#checkpoint机制" class="headerlink" title="checkpoint机制"></a>checkpoint机制</h2><p>checkpoint触发是需要条件的，这些用户可以配置：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 检查触发条件是否满足的频率，60秒 --&gt;</span></span><br><span class="line">dfs.namenode.checkpoint.check.period=60  </span><br><span class="line">dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span><br><span class="line"><span class="comment">&lt;!-- 以上两个参数做checkpoint操作时，secondary namenode的本地工作目录 --&gt;</span></span><br><span class="line">dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;</span><br><span class="line"><span class="comment">&lt;!-- 最大重试次数 --&gt;</span></span><br><span class="line">dfs.namenode.checkpoint.max-retries=3  </span><br><span class="line"><span class="comment">&lt;!-- 两次checkpoint之间的时间间隔3600秒 --&gt;</span></span><br><span class="line">dfs.namenode.checkpoint.period=3600  </span><br><span class="line"><span class="comment">&lt;!-- 两次checkpoint之间最大的操作记录 --&gt;</span></span><br><span class="line">dfs.namenode.checkpoint.txns=1000000</span><br></pre></td></tr></table></figure></p>
<p>具体的流程如下：</p>
<ol>
<li>secondary namenode向namenode请求，询问是否需要进行checkpoint</li>
<li>namenode检查是否满足checkpoint条件，假设满足</li>
<li>secondary namenode发起checkpoint的请求</li>
<li>namenode立即滚动一次正在写的edit文件(edits_inprogress)</li>
<li>secondary namenode将namenode上的edit文件和fsimage文件下载到本地，并加载到内存进行合并，然后在dump成一个新的fsimage文件</li>
<li>secondary namenode将新的fsimage文件上传到namenode，覆盖原有fsimage文件</li>
</ol>
<h1 id="hadoop内部rpc框架使用"><a href="#hadoop内部rpc框架使用" class="headerlink" title="hadoop内部rpc框架使用"></a>hadoop内部rpc框架使用</h1><p>hadoop内部各节点之间通信都是通过rpc进行的，这里使用hadoop内部提供的rpc框架编写一个客户端调用服务端方法的例子。</p>
<p>首先服务端需要将待发布的服务封装成一个接口，我们来自定义这个接口:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">ClientNamenodeProtocol</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 这个versionID字段必须要有，而且名字必须是versionID</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> versionID = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getMetaData</span><span class="params">(String path)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后服务端需要写一个接口的实现类，并将其发布成服务，这里为了简便直接将服务端写成了实现类:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PublishService</span> <span class="keyword">implements</span> <span class="title">ClientNamenodeProtocol</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 创建一个Builder，用来设置服务端信息</span></span><br><span class="line">        RPC.Builder builder = <span class="keyword">new</span> RPC.Builder(<span class="keyword">new</span> Configuration());</span><br><span class="line">        builder.setBindAddress(<span class="string">"127.0.0.1"</span>).setPort(<span class="number">1314</span>)   <span class="comment">// 绑定服务端ip和端口</span></span><br><span class="line">                .setProtocol(ClientNamenodeProtocol.class)  <span class="comment">// 设置服务端的协议，也就是服务接口</span></span><br><span class="line">                .setInstance(<span class="keyword">new</span> PublishService());         <span class="comment">// 设置实现类的实例</span></span><br><span class="line">        RPC.Server server = builder.build();</span><br><span class="line">        <span class="comment">// 启动服务端并发布服务</span></span><br><span class="line">        server.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 服务端实现接口</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getMetaData</span><span class="params">(String path)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> path+<span class="string">"[blk_01,blk_02,blk_03] &#123;blk_01:node01,node02&#125;"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>至此，服务端就简单的将一个getMetaData的方法发布为服务，供其他节点去调用，剩下的就是客户端调用了:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Client</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取代理对象</span></span><br><span class="line">        ClientNamenodeProtocol proxy = RPC.getProxy(</span><br><span class="line">            ClientNamenodeProtocol.class,   <span class="comment">// 设置服务端的协议，也就是服务接口</span></span><br><span class="line">            <span class="number">0L</span>,                             <span class="comment">// 服务端协议的versionID</span></span><br><span class="line">            <span class="keyword">new</span> InetSocketAddress(<span class="string">"127.0.0.1"</span>, <span class="number">1314</span>), <span class="comment">// 服务端的ip和端口</span></span><br><span class="line">            <span class="keyword">new</span> Configuration());</span><br><span class="line">        <span class="comment">// 得到代理对象后，调用服务接口的方法，请求服务端</span></span><br><span class="line">        String metaData = proxy.getMetaData(<span class="string">"/input"</span>);</span><br><span class="line">        System.out.println(metaData);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/09/web架构/HDFS读写流程详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/09/web架构/HDFS读写流程详解/" itemprop="url">HDFS读写流程源码探究</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-09T00:00:00+08:00">
                2017-10-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一篇博客中我们大致讲述了hdfs中几个比较重要的角色，namenode、datanode。这篇文章会将hdfs上传和下载文件的过程细化。</p>
<p>在写上传和下载文件之前，先想一个问题，我们知道hdfs的datanode是可以随时动态扩容的，那么datanode是怎么知道自己是属于哪个namonode的？</p>
<p>首先，我们在每一个节点(包括namenode和datanode)的配置文件中都设置了namenode的地址。</p>
<p>而且在namenode第一次启动之前，我们会执行<code>hdfs namenode -format</code>对namenode的数据目录进行格式化，这个格式化其实就是生成一些标识文件和目录。</p>
<p>假设我们配置的namenode的数据目录为/var/hdfsdata，那么在这个路径下的目录如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@ygq hdfsdata] tree</span><br><span class="line">.</span><br><span class="line">└── dfs</span><br><span class="line">    ├── name</span><br><span class="line">    │   ├── current</span><br><span class="line">    │   │   ├── edits_0000000000000000001-0000000000000000002</span><br><span class="line">    │   │   ├── edits_0000000000000000003-0000000000000000015</span><br><span class="line">    │   │   ├── edits_0000000000000000016-0000000000000000022</span><br><span class="line">    │   │   ├── edits_0000000000000000023-0000000000000000024</span><br><span class="line">    │   │   ├── edits_0000000000000000025-0000000000000000026</span><br><span class="line">    │   │   ├── edits_inprogress_0000000000000000027</span><br><span class="line">    │   │   ├── fsimage_0000000000000000024</span><br><span class="line">    │   │   ├── fsimage_0000000000000000024.md5</span><br><span class="line">    │   │   ├── fsimage_0000000000000000026</span><br><span class="line">    │   │   ├── fsimage_0000000000000000026.md5</span><br><span class="line">    │   │   ├── seen_txid</span><br><span class="line">    │   │   └── VERSION</span><br><span class="line">    │   └── in_use.lock</span><br><span class="line">    └── namesecondary</span><br><span class="line">        ├── current</span><br><span class="line">        │   ├── edits_0000000000000000001-0000000000000000002</span><br><span class="line">        │   ├── edits_0000000000000000003-0000000000000000015</span><br><span class="line">        │   ├── edits_0000000000000000016-0000000000000000022</span><br><span class="line">        │   ├── edits_0000000000000000023-0000000000000000024</span><br><span class="line">        │   ├── edits_0000000000000000025-0000000000000000026</span><br><span class="line">        │   ├── fsimage_0000000000000000024</span><br><span class="line">        │   ├── fsimage_0000000000000000024.md5</span><br><span class="line">        │   ├── fsimage_0000000000000000026</span><br><span class="line">        │   ├── fsimage_0000000000000000026.md5</span><br><span class="line">        │   └── VERSION</span><br><span class="line">        └── in_use.lock</span><br></pre></td></tr></table></figure></p>
<p>我们先不管namesecondary和其他的edits、fsimage文件，在name目录下，有一个VERSION文件，这里面就存放了以当前namenode为首的hdfs集群的基本信息，如集群id等。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat VERSION</span><br><span class="line"></span><br><span class="line">namespaceID=1411893727</span><br><span class="line">clusterID=CID-703e558a-a96c-4e87-9a78-dd8314244fb0</span><br><span class="line">cTime=0</span><br><span class="line">storageType=NAME_NODE</span><br><span class="line">blockpoolID=BP-480178269-192.167.0.74-1515698717067</span><br><span class="line">layoutVersion=-60</span><br></pre></td></tr></table></figure></p>
<p>可以看到有namespaceID，这个主要是实现HA机制的，先不管。然后就是clusterID=CID-703e558a-a96c-4e87-9a78-dd8314244fb0，这个ID就是标识了当前集群，所有将namenode地址配置为本机器的datanode将会自动与此namenode建立联系。</p>
<p>我们再看datanode的数据目录，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以看到datanode数据目录结构和namenode不同，这里不是name和namesecondary，而是data</span></span><br><span class="line"><span class="built_in">cd</span> /var/hdfsdata/dfs/data/current/</span><br><span class="line">cat VERSION</span><br><span class="line"></span><br><span class="line">storageID=DS-c5676a44-1cf7-481d-b748-dbcca2c0ebc5</span><br><span class="line">clusterID=CID-703e558a-a96c-4e87-9a78-dd8314244fb0</span><br><span class="line">cTime=0</span><br><span class="line">datanodeUuid=d6b0d976-0bd0-435f-af57-99df676dc79a</span><br><span class="line">storageType=DATA_NODE</span><br></pre></td></tr></table></figure></p>
<p>可以看到datanode的storageType=DATA_NODE，并且clusterID对应的就是namenode中的clusterID。</p>
<p>所以如果将namenode再执行一次格式化，那么namenode中clusterID和datanode必然不一致了，此时datanode就必然找不到namenode，集群也就没法正常工作了。</p>
<h2 id="HDFS上传文件"><a href="#HDFS上传文件" class="headerlink" title="HDFS上传文件"></a>HDFS上传文件</h2><p>HDFS的上传文件流程比较复杂，我也只能理解一些大体流程，不过我觉得理解大体工作流程对于开发来说是非常有必要的，如果需要再深层次的理解，就需要一行行的调试源码了。具体的文件上传流程如下图所示：<br><img src="/images/hadoop/上传文件.png" alt=""> </p>
<p>当客户端发起上传文件a.txt的请求时(shell客户端或者使用java api上传文件)，大致分为如下几步：</p>
<ol>
<li>通过rpc请求到namenode,请求上传a.txt文件到某目录。然后namenode自身检查文件是否已经存在以及权限认证等，然后返回给客户端是否可上传</li>
<li>假设a.txt比较大，有300M，假设设置的每个block为128M，客户端就会将文件切成3个block。(这里要注意是在客户端切片。)然后客户端再次向namenode发请求，请求上传第一个block。</li>
<li>由于hdfs支持副本机制，所以每一个block可能会存在多个datanode上(但不会同一个datanode上存多个相同的block，因为这样没意义)，具体副本数量由客户端决定，如果客户端没有设置副本数量，则采用服务端默认的副本数量。假设副本数量为3，那么namenode就要返回3个datanode地址给客户端，假设为dn1、dn2、dn3</li>
<li>namenode具体选取哪几个namenode存放这些block，一般要考虑这些datanode的容量，以及网络拓扑(比如优先本地)等</li>
<li><p>客户端拿到datanode地址后，就会与其中一个datanode建立通信连接，然后向此datanode发送数据。<br> 这个发送数据的过程比较复杂，下面一步步来讲，首先我们知道，客户端不止要向1个datanode发送数据，但事实上是这样的吗？事实上，客户端只向一个datanode发送数据，这个datanode(假设为dn1)会自己向dn2建立channel，然后dn2向dn3建立channel(只是一种假设，具体谁和谁建立channel不确定，但可以确定的是从客户端-&gt;多个datanode之间是通过channel流式传输数据的，而客户端确实只传输给1个datanode，这个datanode再传给另外一个datanode，依次建立连接传输数据)</p>
<p> 另外，客户端具体是怎么给dn1发送数据的?上篇博客中我们知道hdfs客户端api分为流式上传和整体上传(比如fs.copyFromLocalFile)，但追踪源码我们知道，哪怕是fs.copyFromLocalFile，其内部也是采用流式传输。通过调试断点得知，具体实现的源码如下。buffSize是什么？从下面的实现中可以看到，客户端一次传输buffSize字节长度数据到datanode，直到传输完毕也就是bytesRead = in.read(buf)=-1.<br> 那么buffSize是多大呢？也就是上图中客户端一个packet的大小是多大？其实这个是配置文件、或者我们java api中客户端设定的io.file.buffer.size来决定的。通过源码我们可以看到具体为<code>conf.getInt(&quot;io.file.buffer.size&quot;, 4096)</code>，也就是默认4K。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//hdfs客户端实现上传文件的核心方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copyBytes</span><span class="params">(InputStream in, OutputStream out, <span class="keyword">int</span> buffSize)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    PrintStream ps = out <span class="keyword">instanceof</span> PrintStream ? (PrintStream)out : <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[buffSize];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> bytesRead = in.read(buf); bytesRead &gt;= <span class="number">0</span>; bytesRead = in.read(buf)) &#123;</span><br><span class="line">        out.write(buf, <span class="number">0</span>, bytesRead);</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span> &amp;&amp; ps.checkError()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Unable to write to output stream."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>当客户端将第一个block的一个个的packet发送完之后，会按照同样的方式再次发送第二个block，直到发送完所有数据。</p>
</li>
</ol>
<h2 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h2><p>HDSF读取文件流程比上传文件稍微简单些。具体的核心实现方法和上传一样，都是<code>public static void copyBytes(InputStream in, OutputStream out, int buffSize)</code>分成多个packet流式上传。具体的步骤如下图所示：</p>
<p><img src="/images/hadoop/读文件.png" alt=""> </p>
<p>当客户端发起下载文件a.txt的请求时(shell客户端或者使用java api下载文件)，大致分为如下几步：</p>
<ol>
<li><p>通过rpc请求到namenode，请求下载a.txt文件，然后namenode自身检查文件是否存在以及权限等，如果存在，则返回给客户端这些文件的block在哪些datanode上。假设返回的数据为:</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;BLK_1,BLK_2,BLK3&#125; </span><br><span class="line">&#123;BLK1:DN1,DN2,DN3 BLK_2:DN1,DN3,DN4 ...&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据namenode的返回结果，客户端就知道了要下载的文件有多少block，并且这些block各在哪些datanode上，假设在dn1、dn3上</p>
</li>
<li>客户端向dn1发起请求，请求下载BLK_1,具体的下载过程和上小节中上传文件是完全一样的，也是使用copyBytes流式获取数据。</li>
<li>当客户端BLK_1下载完成后，再向dn3请求下一个block，依次请求完全部的block之后，在客户端直接拼接(多个block直接追加合并即可，因为之前客户端切分就是直接按字节切分的)成完整文件。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/08/web架构/HDFS集群安装与配置、原理详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ygqqq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="for the dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/08/web架构/HDFS集群安装与配置、原理详解/" itemprop="url">HDFS集群安装与配置、原理详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-08T00:00:00+08:00">
                2017-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="HDFS集群安装与配置"><a href="#HDFS集群安装与配置" class="headerlink" title="HDFS集群安装与配置"></a>HDFS集群安装与配置</h1><p>标题之所以没有叫hadoop是因为本文只将HDFS的部署和运行机制。从Hadoop2.x版本以后，Hadoop的核心组件分为HDFS、MapReduce、Yarn。</p>
<p>其中，HDFS只负责分布式文件存储；MapReduce只负责具体job的分布式离线计算，比如统计对存储在HDFS上的文件中的数据进行统计、排序等等操作；Yarn负责对这些job进行调度。</p>
<p>安装hadoop时，会将这三个组件全部安装，并且提供了各自的配置文件。不过我们可以选择性的只启动某个组件。</p>
<p>这里之所以只写HDFS，也是因为个人觉得HDFS更重要一些。不管是什么样的架构和系统，必然少不了一个分布式文件存储系统，当然HDFS也不是唯一选择，由于HDFS自身的一些特性，很多场景并不适合使用HDFS，这个我们后面再谈。不管是mapreduce、storm还是spark，都可以基于hdfs进行相关处理。而mapreduce和yarn则未必。</p>
<p>下面就参考hadoop官网给出的文档来写下如何安装与配置一个hdfs的集群，本篇文章只讲单namenode模式，联邦机制下的hdfs如果有时间的话就单独再写一篇文章来具体讲。从配置和运行机制上来说，联邦机制下的hdfs都要麻烦不少，而且所需的机器更多，实验环境下不太好部署，少说也得5 6台机器。而且对于开发人员来说，只要配置文件写好，底层hdfs不管是不是运行在联邦机制下，程序代码都是一样的，这些相对来说更偏重运维一些。</p>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>这里采用三台虚拟机进行实验部署，分别为node0、node1、node2。其中，node0作为namenode和second namenode，其余2台作为datanode。至于什么是namenode和datanode，以及hdfs集群的运作方式，后面会很详细的讲清楚，这里先让集群跑起来再说把！</p>
<p>当然，由于要部署集群，所以各节点之间时间最好要同步；另外，各节点之间最好配置了ssh免密登录。如果要使用hadoop提供的脚本来启动集群的话，就必须配置免密登录。</p>
<p>另外，hadoop是java写的，必然也需要安装jdk环境。</p>
<p>然后是下载安装包，这里以hadoop-2.6.5.tar.gz为例，可以到官网去下载压缩包。这里说的压缩包是以及打包编译好的，而不是源码包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xf hadoop-2.6.5.tar.gz -C /usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure>
<p>解压完成后，需要配置一些环境变量和启动参数，这里就参照官网的配置一切从简，大多数参数采用默认即可。这里只说几个必须要配置的参数。</p>
<p>首先，配置HADOOP_HOME环境变量。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export HADOOP_HOME=/usr/local/hadoop-2.6.5/"</span> &gt;&gt; /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将配置文件copy到其他两个节点</span></span><br><span class="line"></span><br><span class="line">scp /etc/profile node1:/etc</span><br><span class="line">scp /etc/profile node2:/etc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在三个节点上均重读配置文件</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></p>
<p>然后，修改hadoop-env.sh文件中的一些参数，这里面可以指定JAVA_HOME，以及指定hadoop运行时的一些参数，比如采用哪种gc，占用多少内存等等的。由于后面我们要使用hadoop提供的脚本来启动hdfs，所以其余节点是通过ssh命令来被执行启动datanode，所以在子shell中会读取不到JAVA_HOME环境变量，必须我们自己指定<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_161/</span><br></pre></td></tr></table></figure></p>
<p>然后，我们需要修改core-site.xml文件来告诉hdfs集群，哪个是namenode，这样当hdfs节点启动时，就会自己去找配置文件中指定的namenode，自动加入集群。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;!--指定namenode的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://node0:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!--用来指定使用hadoop时产生文件的存放目录--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/var/hdfsdata&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"> &lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>当然还有很多很重要的参数，比如io.file.buffer.size来决定文件读写的buffer大小，dfs.blocksize决定文件切片的大小，dfs.namenode.name.dir决定namenode的元数据存放目录等，这里由于只是实验演示，所以就直接采用默认了，生产环境下肯定要根据自身服务器配置进行不同的配置调整。</p>
<p>最后，还有一个文件需要配置，就是slaves，这个文件其实和hadoop自身没什么关系，不配置也能运行，只不过如果想使用hadoop提供的start-dfs.sh脚本时，就会去读取这个文件中写入的主机，并挨个使用ssh命令启动他们，所以我们这里要配置下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br><span class="line"></span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure>
<p>最后，所有配置完成，我们使用scp将这些文件统统拷贝到另外两个节点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r hadoop-2.6.5/ node1:/usr/<span class="built_in">local</span>/</span><br><span class="line"></span><br><span class="line">scp -r hadoop-2.6.5/ node2:/usr/<span class="built_in">local</span>/</span><br></pre></td></tr></table></figure></p>
<p>然后，启动hdfs<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在node0上，也就是我们的namenode上，进行格式化</span></span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用脚本启动所有节点</span></span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>此时，顺利的话，hdfs的3个节点的集群就启动成功了。<br>在node0上使用jps可以看到:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">9088 SecondaryNameNode</span><br><span class="line">9193 Jps</span><br><span class="line">8924 NameNode</span><br></pre></td></tr></table></figure></p>
<p>在node1和node2上可以看到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4882 Jps</span><br><span class="line">4814 DataNode</span><br></pre></td></tr></table></figure></p>
<p>在浏览器里输入<a href="http://node0:50070可以进入一个hadoop提供的web页面，可以查看到当前hdfs集群的运行状况。" target="_blank" rel="noopener">http://node0:50070可以进入一个hadoop提供的web页面，可以查看到当前hdfs集群的运行状况。</a></p>
<h1 id="hdfs的总体概要"><a href="#hdfs的总体概要" class="headerlink" title="hdfs的总体概要"></a>hdfs的总体概要</h1><p>上一节中主要写了hadoop的安装与部署(hdfs模块)，只是初步将集群部署及运行起来，并没有过多的讲述关于hdfs内部运行的一些细节。这一节中，主要讲一些hdfs中的核心名词和概念，也不会太深入涉及hdfs细节，限于文章篇幅，具体的hdfs工作原理放在下一篇博客中写。</p>
<h2 id="hdfs是什么"><a href="#hdfs是什么" class="headerlink" title="hdfs是什么"></a>hdfs是什么</h2><p>首先，hdfs是一个文件系统，是用来存放文件的；其次，hdfs是分布式的，可以用很多台机器来存放和管理用户上传的文件。</p>
<p>既然hdfs是将用户上传的文件保存在多台机器上，那么是不是得有一种机制来记忆到底哪些文件存放在哪些机器上？</p>
<p>另外一个问题，如果用户上传的文件特别大，比如一个用户访问日志、或者一个地图数据文件，动不动上G甚至更大都有可能，如果将这么大的一个文件从客户端上传到HDFS服务器集群，是不是会特别慢!?</p>
<p>而且我们知道，HDFS只是用来做文件存储，供其他系统进行调用的。比如当我们使用mapreduce或者spark程序读取并处理一个特别大的文件，如果这个文件整个的存放在某一台机器上时，是不是就无法充分利用我们的mapreduce和spark分布式的优势了？</p>
<p>基于上述的一些问题，所以才有了hdfs实际的工作原理。</p>
<p>在hdfs中，有两个非常重要的概念，前面我们在部署hdfs的时候也用到了，就是namenode和datanode。</p>
<p>其中datanode只用来存放具体的文件，而namenode负责保存了所有用户上传的文件的元数据信息。比如哪些文件在哪些datanode上面。</p>
<p>而为了解决单个文件过大的问题，hdfs会将用户上传的文件进行切片。将一个大文件切成多个block，分别存放在不同的datanode上，并且会在namenode上记录哪些block存放在哪些datanode上。</p>
<h2 id="使用java-api操作hdfs"><a href="#使用java-api操作hdfs" class="headerlink" title="使用java api操作hdfs"></a>使用java api操作hdfs</h2><p>java api操作hdfs比较简单，基本上和hdfs提供的shell客户端差不多。不过在操作大文件的时候，我们可能要使用流式读取的方式。流式读取的方式也有很多种，可以使用最普通的FileInputStream，也可以使用nio的FileChannel。但不管使用哪种方式，一定要控制好读取的长度和偏移量，防止重读或者漏读数据。</p>
<p>由于安装的hadoop版本是2.6.5版本的，所以使用的是2.6.5版本的hadoop-client，在项目的pom.xml文件中添加如下依赖即可。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            获得FileSystem的实例，这里需要注意，FileSystem是一个抽象类，他有多个实现类</span></span><br><span class="line"><span class="comment">            可以通过指定不同的URI类型来创建不同的FileSystem实现类</span></span><br><span class="line"><span class="comment">            如果不指定URI直接创建，会创建LocalFileSystem本地文件系统操作类，也即file://格式的路径</span></span><br><span class="line"><span class="comment">            这里选择DistributedFileSystem实现类，URI指向我们的namenode的地址和端口</span></span><br><span class="line"><span class="comment">            第三个参数是指定使用哪个用户进行上传，默认会使用当前系统用户。如果用户名和hdfs上目标路径的属主不一致，会抛拒绝访问异常</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://node0:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"root"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将本地目录下某文件上传到hdfs指定目录</span></span><br><span class="line">        fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"/home/ygq/out"</span>), <span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            上述方式是整个将文件上传到hdfs或整个从hdfs下载到本地客户端</span></span><br><span class="line"><span class="comment">            但是如果文件太大，或者想指定文件偏移量，这种方式就不合适了，可以使用流的方式分片上传或下载</span></span><br><span class="line"><span class="comment">            fs.open() 会流式读取hdfs上指定路径的一个文件，并读取一定长度的字节</span></span><br><span class="line"><span class="comment">            每次读多少长度可以自己指定，查看源码可知默认读取io.file.buffer.size设定的长度,可以看到默认读取4096字节</span></span><br><span class="line"><span class="comment">            this.open(f, this.getConf().getInt("io.file.buffer.size", 4096));</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/out"</span>));</span><br><span class="line">        <span class="comment">// 获取文件的元数据信息，比如有多少block，文件大小等等</span></span><br><span class="line">        FileStatus fileStatus = fs.getFileStatus(<span class="keyword">new</span> Path(<span class="string">"/out"</span>));</span><br><span class="line">        <span class="comment">// 获取文件大小，单位是字节</span></span><br><span class="line">        <span class="keyword">long</span> fileLen = fileStatus.getLen();</span><br><span class="line">        <span class="comment">// 记录每次读取到的字节数</span></span><br><span class="line">        <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 记录总共读取到的字节数</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 使用nio的ByteBuffer来流式读取，当然也可以不用nio方式读取，这个都可以</span></span><br><span class="line">        <span class="comment">// 只不过不管用哪种方式读取，都需要控制好读取的长度和偏移量，防止重读或者漏读数据</span></span><br><span class="line">        <span class="comment">// 这里只是将读取到的数据打印到控制台，当然也可以写入本地文件</span></span><br><span class="line">        ByteBuffer byteBuf = ByteBuffer.allocate(<span class="number">4096</span>);</span><br><span class="line">        <span class="keyword">while</span> ((len = in.read(byteBuf)) != -<span class="number">1</span>) &#123;</span><br><span class="line">            sum += len;</span><br><span class="line">            byteBuf.flip();</span><br><span class="line">            <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[byteBuf.remaining()];</span><br><span class="line">            byteBuf.get(buf);</span><br><span class="line">            System.out.print(<span class="keyword">new</span> String(buf, <span class="string">"UTF-8"</span>));</span><br><span class="line">            byteBuf.clear();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">"源文件大小:"</span> + fileLen + <span class="string">"字节,一共读取到:"</span> + sum + <span class="string">"字节"</span>);</span><br><span class="line">        in.close();</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ygqqq</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/javascript.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">37</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ygqqq</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
